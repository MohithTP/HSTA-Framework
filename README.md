# HSTA Multimodal Video Summarization Framework

This project implements a **Hierarchical Spatiotemporal Attention (HSTA)** model for video summarization. Unlike traditional unimodal approaches, this framework utilizes a **Multimodal Fusion** strategy, combining visual features (from MobileNetV3) and audio features (MFCCs) to generate context-aware video summaries.

It includes a fully functional **Flask Web Application** for real-time inference, featuring an interactive UI with attention heatmaps and semantic captions generated by **BLIP**.

---

## ğŸš€ Key Features

*   **Multimodal Intelligence**: Fuses **Visual** (MobileNetV3-Large, 960 dim) and **Audio** (MFCC, 128 dim) features to detect highlights based on both visual cues and audio intensity (e.g., cheering, dialogue).
*   **HSTA Architecture**: A generic Hybrid CNN-RNN model:
    *   **Local Bi-LSTM**: Captures immediate temporal context.
    *   **Global Bi-LSTM**: Captures long-term video dependencies.
    *   **Hierarchical Attention**: Selects key segments locally and globally.
*   **Explainable AI**: Provides an "Attention Score" graph to show *why* specific frames were selected.
*   **Semantic Captioning**: Uses Salesforce's **BLIP** model to generate textual descriptions for the summarized segments.
*   **Optimized Pipeline**: Includes Frame Subsampling (~2 FPS) and In-Memory Audio Extraction for rapid processing (~15x speedup).

---

## ğŸ“Š Dataset

The model is benchmarked and compatible with standard video summarization datasets.

### **Primary Dataset: TVSum (Title-based Video Summarization)**
*   **Source**: [ydata-tvsum50](https://github.com/yalesong/tvsum)
*   **Content**: 50 videos from various genres (news, documentaries, vlogs).
*   **Format**: We support loading ground truth from `ydata-tvsum50.mat` for evaluation.

### **Custom Training**
The framework supports self-supervised fine-tuning on **custom video collections**. You can point the training script to any folder containing `.mp4` files, and it will learn to summarize them by optimizing for **Sparsity** (keeping ~15% of frames) and **Diversity** (avoiding redundancy).

---

## ğŸ› ï¸ Installation

1.  **Clone the Repository**
    ```bash
    git clone https://github.com/MohithTP/HSTA-Framework.git
    cd HSTA-Framework
    ```

2.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```
    *Dependencies include: `torch`, `flask`, `opencv-python`, `librosa`, `transformers`, `moviepy`.*

---

## ğŸ’» Usage

### 1. Run the Web App (Inference)
Launch the Flask dashboard to summarize videos via a Drag-and-Drop interface.
```bash
python flask_app.py
```
*   Open your browser at `http://localhost:5000`
*   Upload a video to see the Summary, Attention Graph, and Captions.

### 2. Fine-Tune the Model (Training)
Train the HSTA model on your own dataset (or the TVSum dataset).
```bash
# Train on the first 20 videos in the 'data' folder
python train.py --folder "data" --limit 20 --epochs 20
```
*   The fine-tuned weights will be saved to `models/hsta_multimodal.pth`.

---

## ğŸ“‚ Project Structure

```
HSTA-Framework/
â”œâ”€â”€ flask_app.py            # Main Web Application entry point
â”œâ”€â”€ train.py                # Self-supervised training script
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hybrid_summarizer.py # HSTA Model Architecture (Bi-LSTM + Attention)
â”‚   â””â”€â”€ feature_extractor.py # MobileNetV3 Visual Feature Extractor
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ summarizer.py       # core logic: Feature Fusion, Inference, BLIP
â”‚   â””â”€â”€ audio_extractor.py  # Librosa/MoviePy Audio Processing
â”œâ”€â”€ templates/              # HTML frontend
â”œâ”€â”€ static/                 # CSS/JS and temporary frames
â””â”€â”€ requirements.txt        # Project dependencies
```

## ğŸ“œ License
This project is open-source.
